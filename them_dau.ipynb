{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os \n",
    "import math\n",
    "import torchtext.data\n",
    "import torchtext.datasets\n",
    "import torchtext.vocab\n",
    "from config import opt\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 9235.88it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_tone_line(utf8_str):\n",
    "    intab_l = \"ạảãàáâậầấẩẫăắằặẳẵóòọõỏôộổỗồốơờớợởỡéèẻẹẽêếềệểễúùụủũưựữửừứíìịỉĩýỳỷỵỹđ\"\n",
    "    intab_u = \"ẠẢÃÀÁÂẬẦẤẨẪĂẮẰẶẲẴÓÒỌÕỎÔỘỔỖỒỐƠỜỚỢỞỠÉÈẺẸẼÊẾỀỆỂỄÚÙỤỦŨƯỰỮỬỪỨÍÌỊỈĨÝỲỶỴỸĐ\"\n",
    "    intab = list(intab_l+intab_u)\n",
    "\n",
    "    outtab_l = \"a\"*17 + \"o\"*17 + \"e\"*11 + \"u\"*11 + \"i\"*5 + \"y\"*5 + \"d\"\n",
    "    outtab_u = \"A\"*17 + \"O\"*17 + \"E\"*11 + \"U\"*11 + \"I\"*5 + \"Y\"*5 + \"D\"\n",
    "    outtab = outtab_l + outtab_u\n",
    "    # Khởi tạo regex tìm kiếm các vị trí nguyên âm có dấu 'ạ|ả|ã|...'\n",
    "    r = re.compile(\"|\".join(intab))\n",
    "\n",
    "    # Dictionary có key-value là từ có dấu-từ không dấu. VD: {'â' : 'a'}\n",
    "    replaces_dict = dict(zip(intab, outtab))\n",
    "    # Thay thế các từ có dấu xuất hiện trong tìm kiếm của regex bằng từ không dấu tương ứng\n",
    "    non_dia_str = r.sub(lambda m: replaces_dict[m.group(0)], utf8_str)\n",
    "    return non_dia_str\n",
    "\n",
    "class tokenize(object):\n",
    "    def __init__(self, param : str) -> None:\n",
    "        self.param = param\n",
    "\n",
    "    def tokenizer(self, sentence):\n",
    "        if self.param == \"with_accents\":\n",
    "            tokens = re.findall(r'\\w+|[^\\w\\s]', sentence, re.UNICODE)\n",
    "            return tokens\n",
    "        \n",
    "        if self.param ==  \"without_accents\":\n",
    "            sentence_ipt = remove_tone_line(sentence)\n",
    "            tokens = re.findall(r'\\w+|[^\\w\\s]', sentence_ipt, re.UNICODE)\n",
    "            return tokens\n",
    "\n",
    "specials = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "\n",
    "def load_dataset(config):\n",
    "    print('Loading dataset...')\n",
    "    tokenize_opt = tokenize(config[\"opt\"])\n",
    "    tokenize_ipt = tokenize(config[\"ipt\"])\n",
    "    train_dataset_ipt = []\n",
    "    train_dataset_opt = []\n",
    "    val_dataset_ipt = []\n",
    "    val_dataset_opt = []\n",
    "    test_dataset_ipt = []\n",
    "    test_dataset_opt = []\n",
    "    counter_opt = Counter()\n",
    "    counter_ipt = Counter()\n",
    "    counter_opt.update(specials)\n",
    "    counter_ipt.update(specials)\n",
    "    with open(config['filename'], 'r', encoding='utf-8') as f: \n",
    "        for i in tqdm(range(config['max_len_load'])):\n",
    "            line = f.readline()\n",
    "            [_, origin_seq] = line.split('\\t')\n",
    "            line_opt = tokenize_opt.tokenizer(origin_seq)\n",
    "            line_ipt = tokenize_ipt.tokenizer(origin_seq)\n",
    "            line_opt = line_opt[:(config['seq_len']-2)]\n",
    "            line_ipt = line_ipt[:(config['seq_len']-2)]\n",
    "            counter_opt.update(line_opt)\n",
    "            counter_ipt.update(line_ipt)\n",
    "            if i < config['train_size']:           \n",
    "                train_dataset_opt.append(line_opt)\n",
    "                train_dataset_ipt.append(line_ipt)\n",
    "            elif i < config['train_size'] + config['val_size']:\n",
    "                val_dataset_opt.append(line_opt)\n",
    "                val_dataset_ipt.append(line_ipt)\n",
    "            else:\n",
    "                test_dataset_opt.append(line_opt)\n",
    "                test_dataset_ipt.append(line_ipt)\n",
    "\n",
    "    \n",
    "\n",
    "    vocab_opt = torchtext.vocab.Vocab(counter_opt, min_freq=1)\n",
    "    vocab_ipt = torchtext.vocab.Vocab(counter_ipt, min_freq=1)\n",
    "    return train_dataset_ipt, train_dataset_opt, val_dataset_ipt, val_dataset_opt, test_dataset_ipt, test_dataset_opt, tokenize_ipt, tokenize_opt, vocab_ipt, vocab_opt\n",
    "\n",
    "\n",
    "train_dataset_ipt, train_dataset_opt, val_dataset_ipt, val_dataset_opt, test_dataset_ipt, test_dataset_opt, tokenize_ipt, tokenize_opt, vocab_ipt, vocab_opt = load_dataset(opt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(vocab_ipt.stoi['<pad>'])\n",
    "print(vocab_opt.stoi['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13849\n",
      "17872\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_ipt.stoi))\n",
    "print(len(vocab_opt.stoi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Internet', 'Society', 'hay', 'ISOC', 'la', 'mot', 'to', 'chuc', 'quoc', 'te', 'hoat', 'dong', 'phi', 'loi', 'nhuan', ',', 'phi', 'chinh', 'phu', 'va', 'bao', 'gom', 'cac', 'thanh', 'vien', 'co', 'trinh', 'do', 'chuyen', 'nganh', '.', 'To', 'chuc', 'nay', 'chu', 'trong', 'den', ':', 'tieu', 'chuan', ',', 'giao', 'duc', 'va', 'cac', 'van', 'de', 've', 'chinh', 'sach', '.', 'Voi', 'tren', '145', 'to', 'chuc', 'thanh', 'vien', 'va', '65', '.', '000', 'thanh', 'vien', 'ca', 'nhan', ',', 'ISOC', 'bao', 'gom', 'nhung', 'con', 'nguoi', 'cu', 'the', 'trong', 'cong', 'dong', 'Internet', '.', 'Moi', 'chi', 'tiet', 'co', 'the', 'tim', 'thay', 'tai', 'website', 'cua', 'ISOC', '.']\n",
      "['Internet', 'Society', 'hay', 'ISOC', 'là', 'một', 'tổ', 'chức', 'quốc', 'tế', 'hoạt', 'động', 'phi', 'lợi', 'nhuận', ',', 'phi', 'chính', 'phủ', 'và', 'bao', 'gồm', 'các', 'thành', 'viên', 'có', 'trình', 'độ', 'chuyên', 'ngành', '.', 'Tổ', 'chức', 'này', 'chú', 'trọng', 'đến', ':', 'tiêu', 'chuẩn', ',', 'giáo', 'dục', 'và', 'các', 'vấn', 'đề', 'về', 'chính', 'sách', '.', 'Với', 'trên', '145', 'tổ', 'chức', 'thành', 'viên', 'và', '65', '.', '000', 'thành', 'viên', 'cá', 'nhân', ',', 'ISOC', 'bao', 'gồm', 'những', 'con', 'người', 'cụ', 'thể', 'trong', 'cộng', 'đồng', 'Internet', '.', 'Mọi', 'chi', 'tiết', 'có', 'thể', 'tìm', 'thấy', 'tại', 'website', 'của', 'ISOC', '.']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_ipt[2])\n",
    "print(train_dataset_opt[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(x, vocab):\n",
    "    return [vocab.stoi[s] for s in x]\n",
    "\n",
    "def decode(x, vocab):\n",
    "    return [vocab.itos[s] for s in x]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(size):\n",
    "    '''\n",
    "    mask được sử dụng cho quá trình dự đoán, mô hình không thấy được tương lai\n",
    "    '''\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int64).to(device)\n",
    "    return mask == 0\n",
    "\n",
    "a = (torch.tensor([1, 2]) != 2).long().to(device) & causal_mask(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, train_dataset_ipt, train_dataset_opt, vocab_ipt, vocab_opt, config):\n",
    "        super().__init__()\n",
    "        self.seq_len = config['seq_len']\n",
    "\n",
    "        self.train_dataset_ipt = train_dataset_ipt\n",
    "        self.train_dataset_opt = train_dataset_opt\n",
    "\n",
    "        self.vocab_ipt = vocab_ipt\n",
    "        self.vocab_opt = vocab_opt\n",
    "\n",
    "        self.sos_token_ipt = torch.tensor([vocab_ipt.stoi['<sos>']], dtype=torch.int64, device=device)\n",
    "        self.eos_token_ipt = torch.tensor([vocab_ipt.stoi['<eos>']], dtype=torch.int64, device=device)\n",
    "        self.pad_token_ipt = torch.tensor([vocab_ipt.stoi['<pad>']], dtype=torch.int64, device=device)\n",
    "\n",
    "        self.sos_token_opt = torch.tensor([vocab_opt.stoi['<sos>']], dtype=torch.int64, device=device)\n",
    "        self.eos_token_opt = torch.tensor([vocab_opt.stoi['<eos>']], dtype=torch.int64, device=device)\n",
    "        self.pad_token_opt = torch.tensor([vocab_opt.stoi['<pad>']], dtype=torch.int64, device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_dataset_ipt)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        ipt_tokenized = train_dataset_ipt[index]\n",
    "        opt_tokenized = train_dataset_opt[index]\n",
    "\n",
    "        enc_num_padding_tokens = self.seq_len - len(ipt_tokenized) - 2\n",
    "        dec_num_padding_tokens = self.seq_len - len(opt_tokenized) - 1\n",
    "\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too long! Try to increase seq_len in config.py\")\n",
    "        \n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token_ipt,\n",
    "                torch.tensor(encode(ipt_tokenized, self.vocab_ipt), dtype=torch.int64, device=device),\n",
    "                self.eos_token_ipt,\n",
    "                torch.tensor([self.pad_token_ipt] * enc_num_padding_tokens, dtype=torch.int64, device=device)\n",
    "            ],\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token_opt,\n",
    "                torch.tensor(encode(opt_tokenized, self.vocab_opt), dtype=torch.int64, device=device),\n",
    "                torch.tensor([self.pad_token_opt] * dec_num_padding_tokens, dtype=torch.int64, device=device)\n",
    "            ],\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(encode(opt_tokenized, self.vocab_opt), dtype=torch.int64, device=device),\n",
    "                self.eos_token_opt,\n",
    "                torch.tensor([self.pad_token_opt] * dec_num_padding_tokens, dtype=torch.int64, device=device)\n",
    "            ],\n",
    "            dim=0\n",
    "        )\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input, # (seq_len)\n",
    "            \"decoder_input\": decoder_input, # (seq_len)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token_ipt).unsqueeze(0).unsqueeze(0).long().to(device), # (1, seq_len) do input (batch, seq_len, d_model)\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token_opt).unsqueeze(0).long().to(device) & causal_mask(decoder_input.size(0)), # (seq_len, seq_len)\n",
    "            \"label\": label, # (seq_len)\n",
    "            # \"ipt_tokenized\": ipt_tokenized, \n",
    "            # \"opt_tokenized\": opt_tokenized\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 200])\n",
      "torch.Size([1, 200, 200])\n"
     ]
    }
   ],
   "source": [
    "test1 = CustomDataset(train_dataset_ipt, train_dataset_opt, vocab_ipt, vocab_opt, opt)\n",
    "print(test1.__getitem__(1)[\"encoder_mask\"].shape)\n",
    "print(test1.__getitem__(1)[\"decoder_mask\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config, ipt_vocab_len, opt_vocab_len):\n",
    "    model = build_transformer(ipt_vocab_len, opt_vocab_len, config['seq_len'], config['seq_len'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipt_vocab_len = len(vocab_ipt.stoi)\n",
    "opt_vocab_len = len(vocab_opt.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds(config):\n",
    "    train_ds = CustomDataset(train_dataset_ipt=train_dataset_ipt,\n",
    "                             train_dataset_opt=train_dataset_opt,\n",
    "                             vocab_ipt=vocab_ipt,\n",
    "                             vocab_opt=vocab_opt,\n",
    "                             config=config)\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "    return train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    train_dataloader= get_ds(config)\n",
    "    model = get_model(config, ipt_vocab_len=ipt_vocab_len, opt_vocab_len=opt_vocab_len).to(device)\n",
    "    print(\"Using device: \", device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=vocab_opt.stoi['<pad>'], label_smoothing=0.1).to(device)\n",
    "\n",
    "    for epoch in range(0, config['num_epochs']):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f'Processing Epoch {epoch:02d}')\n",
    "        for batch in batch_iterator:\n",
    "\n",
    "            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
    "\n",
    "            # Run the tensors through the encoder, decoder and the projection layer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
    "            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n",
    "\n",
    "            # Compare the output with the label\n",
    "            label = batch['label'].to(device) # (B, seq_len)\n",
    "\n",
    "            # Compute the loss using a simple cross entropy\n",
    "            loss = loss_fn(proj_output.view(-1, opt_vocab_len), label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Using device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00:   0%|          | 0/2000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (8) must match the existing size (4) at non-singleton dimension 1.  Target sizes: [4, 8, 200, 200].  Tensor sizes: [4, 1, 200]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      3\u001b[0m     config \u001b[38;5;241m=\u001b[39m opt\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[57], line 20\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     17\u001b[0m decoder_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# (B, 1, seq_len, seq_len)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Run the tensors through the encoder, decoder and the projection layer\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_mask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, seq_len, d_model)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecode(encoder_output, encoder_mask, decoder_input, decoder_mask) \u001b[38;5;66;03m# (B, seq_len, d_model)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m proj_output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mproject(decoder_output) \u001b[38;5;66;03m# (B, seq_len, vocab_size)\u001b[39;00m\n",
      "File \u001b[1;32md:\\Summarize\\them_dau\\model.py:214\u001b[0m, in \u001b[0;36mTransformer.encode\u001b[1;34m(self, src, src_mask)\u001b[0m\n\u001b[0;32m    212\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_embed(src)\n\u001b[0;32m    213\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_pos(src)\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\caohu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\caohu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Summarize\\them_dau\\model.py:158\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 158\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[1;32mc:\\Users\\caohu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\caohu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Summarize\\them_dau\\model.py:145\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[1;34m(self, x, src_mask)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, src_mask):\n\u001b[1;32m--> 145\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual_connections\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_connections[\u001b[38;5;241m1\u001b[39m](x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_block)\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\caohu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\caohu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Summarize\\them_dau\\model.py:81\u001b[0m, in \u001b[0;36mResidualConnection.forward\u001b[1;34m(self, x, sublayer)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, sublayer):\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[43msublayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\Summarize\\them_dau\\model.py:145\u001b[0m, in \u001b[0;36mEncoderBlock.forward.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, src_mask):\n\u001b[1;32m--> 145\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_connections[\u001b[38;5;241m0\u001b[39m](x, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    146\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_connections[\u001b[38;5;241m1\u001b[39m](x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_block)\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\caohu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\caohu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Summarize\\them_dau\\model.py:126\u001b[0m, in \u001b[0;36mMultiHeadAttentionBlock.forward\u001b[1;34m(self, q, k, v, mask)\u001b[0m\n\u001b[0;32m    123\u001b[0m value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mview(value\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], value\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# Calculate attention\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mMultiHeadAttentionBlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Combine all the heads together\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\u001b[39;00m\n\u001b[0;32m    130\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\n",
      "File \u001b[1;32md:\\Summarize\\them_dau\\model.py:107\u001b[0m, in \u001b[0;36mMultiHeadAttentionBlock.attention\u001b[1;34m(query, key, value, mask, dropout)\u001b[0m\n\u001b[0;32m    104\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m (query \u001b[38;5;241m@\u001b[39m key\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(d_k)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;66;03m# Write a very low value (indicating -inf) to the positions where mask == 0\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m     \u001b[43mattention_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1e9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m attention_scores\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (batch, h, seq_len, seq_len) # Apply softmax\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dropout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (8) must match the existing size (4) at non-singleton dimension 1.  Target sizes: [4, 8, 200, 200].  Tensor sizes: [4, 1, 200]"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "if __name__ ==  '__main__':\n",
    "    config = opt\n",
    "    train_model(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wikipedia', 'co', 'vai', 'du', 'an', 'lien', 'quan', ':']\n",
      "['Wikipedia', 'có', 'vài', 'dự', 'án', 'liên', 'quan', ':']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_ipt[5214])\n",
    "print(train_dataset_opt[5214])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia co vai du an lien quan:\n"
     ]
    }
   ],
   "source": [
    "word1 = remove_tone_line(\"Wikipedia có vài dự án liên quan:\")\n",
    "print(word1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10938, 10330, 12711, 13437, 14432, 13314, 17862, 13372, 11034,  7607],\n",
      "       device='cuda:0')\n",
      "Braj\n",
      "90377\n",
      "Joey\n",
      "Minxin\n",
      "Rèn\n",
      "Mascot\n",
      "Ắc\n",
      "Megabus\n",
      "Búp\n",
      "Anouvong\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# train_dataset_ipt, train_dataset_opt, val_dataset_ipt, val_dataset_opt, test_dataset_ipt, test_dataset_opt, tokenize_ipt, tokenize_opt, vocab_ipt, vocab_opt\n",
    "\n",
    "from pathlib import Path\n",
    "from config import opt\n",
    "from model import build_transformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "\n",
    "\n",
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0\n",
    "\n",
    "def translate(config, sentence : str):\n",
    "    seq_len = config['seq_len']\n",
    "\n",
    "    sos_token_ipt = torch.tensor([vocab_ipt.stoi['<sos>']], dtype=torch.int64, device=device)\n",
    "    eos_token_ipt = torch.tensor([vocab_ipt.stoi['<eos>']], dtype=torch.int64, device=device)\n",
    "    pad_token_ipt = torch.tensor([vocab_ipt.stoi['<pad>']], dtype=torch.int64, device=device)\n",
    "\n",
    "    sos_token_opt = torch.tensor([vocab_opt.stoi['<sos>']], dtype=torch.int64, device=device)\n",
    "    eos_token_opt = torch.tensor([vocab_opt.stoi['<eos>']], dtype=torch.int64, device=device)\n",
    "    pad_token_opt = torch.tensor([vocab_opt.stoi['<pad>']], dtype=torch.int64, device=device)\n",
    "\n",
    "    model = get_model(config, ipt_vocab_len=ipt_vocab_len, opt_vocab_len=opt_vocab_len).to(device)\n",
    "    state = torch.load('model.pt')\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        source = tokenize_ipt.tokenizer(sentence)\n",
    "        enc_num_padding_tokens = seq_len - len(source) - 2\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                sos_token_ipt,\n",
    "                torch.tensor(encode(source, vocab_ipt), dtype=torch.int64, device=device),\n",
    "                eos_token_ipt,\n",
    "                torch.tensor([pad_token_ipt] * enc_num_padding_tokens, dtype=torch.int64, device=device)\n",
    "            ],\n",
    "            dim=0\n",
    "        ).unsqueeze(0).to(device) # --> (1, seq_len)\n",
    "        encoder_mask = (encoder_input != sos_token_ipt).long().to(device)\n",
    "        encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "        # # print(encoder_output)\n",
    "        decoder_output = []\n",
    "        decoder_input = torch.cat([\n",
    "            sos_token_opt,\n",
    "            torch.tensor(decoder_output, dtype=torch.int64, device=device),\n",
    "            torch.tensor([pad_token_opt] * (seq_len - len(decoder_output) - 1), dtype=torch.int64, device=device),\n",
    "        ], dim=0).unsqueeze(0).to(device)\n",
    "        decoder_mask = (decoder_input != pad_token_opt).unsqueeze(0).unsqueeze(0).long().to(device)\n",
    "        out = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask).to(device)\n",
    "        prob = model.project(out[0][-1])\n",
    "        _, next_word = torch.topk(prob, dim = 0, k=10)\n",
    "        print(next_word)\n",
    "        for x in next_word:\n",
    "            print(f\"{vocab_opt.itos[x.item()]}\")\n",
    "    #     while len(decoder_output) < seq_len:\n",
    "    #         decoder_input = torch.cat([\n",
    "    #             sos_token_opt,\n",
    "    #             torch.tensor(decoder_output, dtype=torch.int64, device=device),\n",
    "    #             torch.tensor([pad_token_opt] * (seq_len - len(decoder_output) - 1), dtype=torch.int64, device=device),\n",
    "    #         ], dim=0).unsqueeze(0).to(device)\n",
    "    #         decoder_mask = (decoder_input != pad_token_opt).long().to(device)\n",
    "    #         out = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask).to(device)\n",
    "    #         prob = model.project(out)\n",
    "    #         prob = prob[0][-1]\n",
    "    #         _, next_word = torch.topk(prob, dim=0)\n",
    "    #         # print(next_word.item())\n",
    "    #         # print(vocab_opt.stoi['<eos>'])\n",
    "    #         # break\n",
    "    #         if next_word.item() == vocab_opt.stoi['<eos>']:\n",
    "    #             break\n",
    "\n",
    "    #         decoder_output.append(next_word.item())\n",
    "    #         #print(decoder_output)\n",
    "    #         #print(decoder_output)\n",
    "    #         # decoder_input = torch.cat([\n",
    "    #         #     train_ds.tgt_sos_token,\n",
    "    #         #     torch.tensor(decoder_output, dtype=torch.int64),\n",
    "    #         #     torch.tensor([train_ds.tgt_pad_token] * (train_ds.seq_len - len(decoder_output) - 1), dtype=torch.int64),\n",
    "    #         # ], dim=0).unsqueeze(0).to(device)\n",
    "    #         # print(f\"{train_ds.tokenizer.tokenizer_tgt_id_to_token(next_word.item())}\", end=' ')\n",
    "    #         # if next_word == train_ds.tokenizer.tokenizer_tgt_token_to_id('<eos>'):\n",
    "    #         #     break\n",
    "\n",
    "    # #print(decoder_output)\n",
    "    # return \" \".join(vocab_opt.itos[x] for x in decoder_output)\n",
    "\n",
    "print(translate(opt, 'trien'))\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
